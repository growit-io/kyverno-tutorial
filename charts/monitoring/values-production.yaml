kube-prometheus-stack:

  kube-state-metrics:
    # Ingest our custom team ownership label for all resource types known
    # to kube-state-metrics.
    metricLabelsAllowlist:
      - "*=[example.com/team]"
      # This doesn't work, because KSM needs to be told explicitly how to
      # monitor custom resources. See `customResourceState` below.
      #- "servicemonitors=[example.com/team]"

    # Define custom metrics for CRDs.
    customResourceState:
      enabled: true
      config:
        kind: CustomResourceStateMetrics
        spec:
          resources:
            - groupVersionKind:
                group: monitoring.coreos.com
                kind: ServiceMonitor
                version: "*"
              labelsFromPath:
                namespace: [metadata, namespace]
                name: [metadata, name]
              metrics:
                - name: "labels"
                  help: "Kubernetes labels for ServiceMonitors converted to Prometheus labels."
                  each:
                    type: Info
                    info:
                      path: [metadata, labels]
                      labelsFromPath:
                        label_example_com_team: ["example.com/team"]

    rbac:
      extraRules:
        - apiGroups: ["monitoring.coreos.com"]
          resources: ["servicemonitors"]
          verbs: ["list", "watch"]

    prometheus:
      monitor:
        metricRelabelings:
          # kube-state-metrics prefixes label labels with "label_"... :)
          # The targetLabels for a ServiceMonitor, for example, don't cause
          # this to happen and we should be consistent, so drop the prefix.
          - action: labelmap
            regex: label_(example_com_.+)
            replacement: $1

  defaultRules:
    additionalAggregationLabels:
      - example_com_team
    additionalRuleLabels:
      responder: '{{ or $labels.example_com_team "platform-team" }}'
    disabled:
      # We need to define our own version of the alert in order to include our
      # custom team label in the query expression.
      TargetDown: true
  additionalPrometheusRulesMap:
    overrides:
      groups:
        - name: general
          rules:
            - alert: TargetDown
              expr: |
                100 * (
                  count by (example_com_team, cluster, job, namespace, service) (up == 0) /
                  count by (example_com_team, cluster, job, namespace, service) (up)
                ) > 10
              for: 10m
              labels:
                responder: '{{ or $labels.example_com_team "platform-team" }}'
                severity: warning
              annotations:
                description: '{{ printf "%.4g" $value }}% of the {{ $labels.job }}/{{ $labels.service }} targets in {{ $labels.namespace }} namespace are down.'
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/general/targetdown
                summary: One or more targets are unreachable.

  # We could label all resources of this Helm chart as managed by the platform
  # team, but it isn't strictly necessary with our solution since we can rely on
  # the template expression for "defaultRules.additionalRuleLabels.responder" to
  # fall back to "platform-team" when an alert doesn't have this team label.
  #commonLabels:
  #  example.com/team: platform-team

  #alertmanager:
  #  serviceMonitor:
  #    relabelings:
  #      - action: labelmap
  #        regex: __meta_kubernetes_.+_label_(example_com_.+)
  #        replacement: $1
  #kubeEtcd:
  #  serviceMonitor:
  #    relabelings:
  #      - action: labelmap
  #        regex: __meta_kubernetes_.+_label_(example_com_.+)
  #        replacement: $1

  # XXX: not necessary for this tutorial, but perhaps for deployment-related alerts, in which case joins in the alert query expression are also needed
  #kube-state-metrics:
  #  metricLabelsAllowlist:
  #    - "pods=[example.com/team]"
  #    - "services=[example.com/team]"