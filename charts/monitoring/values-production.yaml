kube-prometheus-stack:

  prometheus:
    prometheusSpec:
      additionalConfig:
        podTargetLabels:
          - example.com/team

  kube-state-metrics:
    # Ingest our custom team ownership label for all resource types known
    # to kube-state-metrics.
    metricLabelsAllowlist:
      - "*=[example.com/team]"
      # This doesn't work, because KSM needs to be told explicitly how to
      # monitor custom resources. See `customResourceState` below.
      #- "servicemonitors=[example.com/team]"

    # Define custom metrics for CRDs.
    customResourceState:
      enabled: true
      config:
        kind: CustomResourceStateMetrics
        spec:
          resources:
            - groupVersionKind:
                group: monitoring.coreos.com
                kind: ServiceMonitor
                version: "*"
              labelsFromPath:
                namespace: [metadata, namespace]
                name: [metadata, name]
              metrics:
                - name: "labels"
                  help: "Kubernetes labels for ServiceMonitors converted to Prometheus labels."
                  each:
                    type: Info
                    info:
                      path: [metadata, labels]
                      labelsFromPath:
                        label_example_com_team: ["example.com/team"]

    rbac:
      extraRules:
        - apiGroups: ["monitoring.coreos.com"]
          resources: ["servicemonitors"]
          verbs: ["list", "watch"]

    prometheus:
      monitor:
        metricRelabelings:
          # kube-state-metrics prefixes resource label metric labels with
          # "label_", but this doesn't happen with `podTargetLabels` on a
          # PodMonitor or ServiceMonitor. Since it is fairly reundant, to
          # drop this prefix feels somewhat more aesthetically pleasing.
          - action: labelmap
            regex: label_(example_com_.+)
            replacement: $1
          - action: labeldrop
            regex: label_example_com_.+

  defaultRules:
    additionalAggregationLabels:
      - example_com_team

    ## Setting a dynamic responder for alerts could be as simple as this, but
    ## `additionalRuleLabels` has the side-effect of adding this label verbatim
    ## to recorded metrics, too. Adding the label to `additionalRuleGroupLabels`
    ## for each alert group individually thus seems to be the better solution.
    ##
    #additionalRuleLabels:
    #  responder: '{{ or $labels.example_com_team "platform-team" }}'

    additionalRuleGroupLabels:
      kubernetesResources:
        responder: '{{ or $labels.example_com_team "platform-team" }}'
      # ...

    disabled:
      # We need to define our own version of the alert in order to include our
      # custom team label in the query expression.
      TargetDown: true

  additionalPrometheusRulesMap:
    overrides:
      groups:
        - name: general
          rules:
            - alert: TargetDown
              expr: |
                100 * (
                  count by (example_com_team, cluster, job, namespace, service) (up == 0) /
                  count by (example_com_team, cluster, job, namespace, service) (up)
                ) > 10
              for: 10m
              labels:
                responder: '{{ or $labels.example_com_team "platform-team" }}'
                severity: warning
              annotations:
                description: '{{ printf "%.4g" $value }}% of the {{ $labels.job }}/{{ $labels.service }} targets in {{ $labels.namespace }} namespace are down.'
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/general/targetdown
                summary: One or more targets are unreachable.

  # We could label all resources of this Helm chart as managed by the platform
  # team, but it isn't strictly necessary with our solution since we can rely on
  # the template expression for "defaultRules.additionalRuleLabels.responder" to
  # fall back to "platform-team" when an alert doesn't have this team label.
  #commonLabels:
  #  example.com/team: platform-team

  #alertmanager:
  #  serviceMonitor:
  #    relabelings:
  #      - action: labelmap
  #        regex: __meta_kubernetes_.+_label_(example_com_.+)
  #        replacement: $1
  #kubeEtcd:
  #  serviceMonitor:
  #    relabelings:
  #      - action: labelmap
  #        regex: __meta_kubernetes_.+_label_(example_com_.+)
  #        replacement: $1